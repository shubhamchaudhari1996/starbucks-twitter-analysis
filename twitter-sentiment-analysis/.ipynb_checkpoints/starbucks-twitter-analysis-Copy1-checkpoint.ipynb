{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Set Connecion\n",
    "\n",
    "#Get from developers.twitter.com/App->Setting->keys&tokens\n",
    "#Just assign the credentials\n",
    "\n",
    "consumer_key = \"xhsSu7y92oAFi9U3yX794Chav\"\n",
    "consumer_secret = \"0YWxdhZ2XvBs6Tk33iolwIYSdcuZL23CmhRK1TeHV6bIMdQ7SM\"\n",
    "access_token = \"324351622-rO9AGKtQ8WvWBQrX8bnWlEvXcJ3FgKpR3tPLB7fW\"\n",
    "access_token_secret = \"8OAe9FSFELuvIRHccwOgncxAgxIPgGvp19JDe44IeYOuV\"\n",
    "\n",
    "\n",
    "\n",
    "# Use the above credentials to authenticate the API.\n",
    "\n",
    "auth = tweepy.OAuthHandler( consumer_key , consumer_secret )\n",
    "auth.set_access_token( access_token , access_token_secret )\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStreamListener(tweepy.StreamListener):\n",
    "    \"\"\"\n",
    "    tweepy.StreamListener is a class provided by tweepy used to access the Twitter \n",
    "    Streaming API. It allows us to retrieve tweets in real time.\n",
    "    \"\"\"\n",
    "    def __init__(self, api):\n",
    "        self.api = api\n",
    "        super(tweepy.StreamListener, self).__init__()\n",
    "        \n",
    "        # Connecting to MongoDB and use the database twitter.\n",
    "        self.db = pymongo.MongoClient().starbucks_analysis\n",
    " \n",
    "    def on_data(self, tweet):\n",
    "        '''\n",
    "        This will be called each time we receive stream data and store the tweets \n",
    "        into the datascience collection.\n",
    "        '''\n",
    "        self.db.tweets.insert(json.loads(tweet))\n",
    " \n",
    "    def on_error(self, status_code):\n",
    "        # This is called when an error occurs\n",
    "        print >> sys.stderr, 'Encountered error with status code:', status_code\n",
    "        return True # Don't kill the stream\n",
    " \n",
    "    def on_timeout(self):\n",
    "        # This is called if there is a timeout\n",
    "        print >> sys.stderr, 'Timeout.....'\n",
    "        return True # Don't kill the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our stream object\n",
    "sapi = tweepy.streaming.Stream(auth, CustomStreamListener(api))\n",
    "\n",
    "'''\n",
    "The track parameter is an array of search terms to stream. In this example I will use \n",
    "filter to stream all tweets containing the hashtags datascience and datascientist.\n",
    "''' \n",
    "sapi.filter(track=['#starbucks'])\n",
    "print(sapi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking: ['#starbucks']\n",
      "An Error has occured: 403\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tweepy\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "\n",
    "MONGO_HOST= 'mongodb://localhost/twitterdb'  # assuming you have mongoDB installed locally\n",
    "                                             # and a database called 'twitterdb'\n",
    "\n",
    "WORDS = ['#starbucks']\n",
    "\n",
    "CONSUMER_KEY = \"xhsSu7y92oAFi9U3yX794Chav\"\n",
    "CONSUMER_SECRET = \"0YWxdhZ2XvBs6Tk33iolwIYSdcuZL23CmhRK1TeHV6bIMdQ7SM\"\n",
    "ACCESS_TOKEN = \"324351622-rO9AGKtQ8WvWBQrX8bnWlEvXcJ3FgKpR3tPLB7fW\"\n",
    "ACCESS_TOKEN_SECRET = \"8OAe9FSFELuvIRHccwOgncxAgxIPgGvp19JDe44IeYOuV\"\n",
    "\n",
    "\n",
    "class StreamListener(tweepy.StreamListener):    \n",
    "    #This is a class provided by tweepy to access the Twitter Streaming API. \n",
    "\n",
    "    def on_connect(self):\n",
    "        # Called initially to connect to the Streaming API\n",
    "        print(\"You are now connected to the streaming API.\")\n",
    " \n",
    "    def on_error(self, status_code):\n",
    "        # On error - if an error occurs, display the error / status code\n",
    "        print('An Error has occured: ' + repr(status_code))\n",
    "        return False\n",
    " \n",
    "    def on_data(self, data):\n",
    "        #This is the meat of the script...it connects to your mongoDB and stores the tweet\n",
    "        try:\n",
    "            client = MongoClient(MONGO_HOST)\n",
    "            \n",
    "            # Use twitterdb database. If it doesn't exist, it will be created.\n",
    "            db = client.twitterdb\n",
    "    \n",
    "            # Decode the JSON from Twitter\n",
    "            datajson = json.loads(data)\n",
    "            \n",
    "            #grab the 'created_at' data from the Tweet to use for display\n",
    "            created_at = datajson['created_at']\n",
    "\n",
    "            #print out a message to the screen that we have collected a tweet\n",
    "            print(\"Tweet collected at \" + str(created_at))\n",
    "            \n",
    "            #insert the data into the mongoDB into a collection called twitter_search\n",
    "            #if twitter_search doesn't exist, it will be created.\n",
    "            db.twitter_search.insert(datajson)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "#Set up the listener. The 'wait_on_rate_limit=True' is needed to help with Twitter API rate limiting.\n",
    "listener = StreamListener(api=tweepy.API(wait_on_rate_limit=True)) \n",
    "streamer = tweepy.Stream(auth=auth, listener=listener)\n",
    "print(\"Tracking: \" + str(WORDS))\n",
    "streamer.filter(track=WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shubham\\anaconda3\\envs\\starbucks-analysis\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: ensure_index is deprecated. Use create_index instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n",
      "403\n",
      "403\n",
      "403\n",
      "403\n",
      "403\n",
      "403\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-6a5e2d8b3914>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\starbucks-analysis\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, follow, track, is_async, locations, stall_warnings, languages, encoding, filter_level)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'filter_level'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_level\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'delimited'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'length'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_async\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     def sitestream(self, follow, stall_warnings=False,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\starbucks-analysis\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_start\u001b[1;34m(self, is_async)\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\starbucks-analysis\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    279\u001b[0m                         self.retry_time = max(self.retry_420_start,\n\u001b[0;32m    280\u001b[0m                                               self.retry_time)\n\u001b[1;32m--> 281\u001b[1;33m                     \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretry_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m                     self.retry_time = min(self.retry_time * 2,\n\u001b[0;32m    283\u001b[0m                                           self.retry_time_cap)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import datetime\n",
    "\n",
    "\n",
    "# The MongoDB connection info. This assumes your database name is twitter_database, and your collection name is tweets.\n",
    "connection = MongoClient('localhost', 27017)\n",
    "db = connection.twitter_database\n",
    "db.tweets.ensure_index(\"id\", unique=True, dropDups=True)\n",
    "collection = db.tweets\n",
    "\n",
    "# Add the keywords you want to track. They can be cashtags, hashtags, or words.\n",
    "keywords = ['starbucks', '#starbucks']\n",
    "\n",
    "# Optional - Only grab tweets of specific language\n",
    "language = ['en']\n",
    "\n",
    "# You need to replace these with your own values that you get after creating an app on Twitter's developer portal.\n",
    "consumer_key = \"xhsSu7y92oAFi9U3yX794Chav\"\n",
    "consumer_secret = \"0YWxdhZ2XvBs6Tk33iolwIYSdcuZL23CmhRK1TeHV6bIMdQ7SM\"\n",
    "access_token = \"324351622-rO9AGKtQ8WvWBQrX8bnWlEvXcJ3FgKpR3tPLB7fW\"\n",
    "access_token_secret = \"8OAe9FSFELuvIRHccwOgncxAgxIPgGvp19JDe44IeYOuV\"\n",
    "\n",
    "# The below code will get Tweets from the stream and store only the important fields to your database\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "\n",
    "        # Load the Tweet into the variable \"t\"\n",
    "        t = json.loads(data)\n",
    "\n",
    "        # Pull important data from the tweet to store in the database.\n",
    "        tweet_id = t['id_str']  # The Tweet ID from Twitter in string format\n",
    "        created_at = t['created_at']\n",
    "        username = t['user']['screen_name']  # The username of the Tweet author\n",
    "        followers = t['user']['followers_count']  # The number of followers the Tweet author has\n",
    "        text = t['text']  # The entire body of the Tweet\n",
    "        favorite_count = t['favorite_count']\n",
    "        hashtags = t['entities']['hashtags']  # Any hashtags used in the Tweet\n",
    "        retweet_count = t['retweet_count']\n",
    "        location = t['user']['location']\n",
    "        dt = t['created_at']  # The timestamp of when the Tweet was created\n",
    "        language = t['lang']  # The language of the Tweet\n",
    "\n",
    "        # Convert the timestamp string given by Twitter to a date object called \"created\". This is more easily manipulated in MongoDB.\n",
    "        created = datetime.datetime.strptime(dt, '%a %b %d %H:%M:%S +0000 %Y')\n",
    "\n",
    "        # Load all of the extracted Tweet data into the variable \"tweet\" that will be stored into the database\n",
    "        tweet = {'id':tweet_id, 'created_at':created_at, 'username':username, 'followers':followers, 'tweet':text, 'favorite_count':favorite_count,'hashtags':hashtags, 'retweet_count':retweet_count, 'location':location, 'language':language, 'created':created}\n",
    "\n",
    "        # Save the refined Tweet data to MongoDB\n",
    "        collection.save(tweet)\n",
    "\n",
    "        # Optional - Print the username and text of each Tweet to your console in realtime as they are pulled from the stream\n",
    "        print(username + ':' + ' ' + text)\n",
    "        return True\n",
    "        \n",
    "    # Prints the reason for an error to your console\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "# Some Tweepy code that can be left alone. It pulls from variables at the top of the script\n",
    "if __name__ == '__main__':\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "    stream = Stream(auth, l)\n",
    "    stream.filter(track=keywords, languages=language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Date, User, IsVerified, Tweet, Likes, RT, User_location]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 3.To get the tweets in a Proper format, first lets create a Dataframe to store the extracted data.\n",
    "\n",
    "df = pd.DataFrame(columns=[\"Date\",\"User\",\"IsVerified\",\"Tweet\",\"Likes\",\"RT\",'User_location'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use api as api.search inside this tweepy cursor.\n",
    "\n",
    "# We will Use **tweepy.cursor()** because we want to extract a larger number of tweets i.e over 100,500 etc\n",
    "\n",
    "def get_tweets(Topic,Count):    \n",
    "    i=0\n",
    "    for tweet in tweepy.Cursor(api.search, q=Topic,count=100, lang=\"en\",exclude='retweets').items():\n",
    "        print(i, end='\\r')\n",
    "        df.loc[i,\"Date\"] = tweet.created_at\n",
    "        df.loc[i,\"User\"] = tweet.user.name\n",
    "        df.loc[i,\"IsVerified\"] = tweet.user.verified\n",
    "        df.loc[i,\"Tweet\"] = tweet.text\n",
    "        df.loc[i,\"Likes\"] = tweet.favorite_count\n",
    "        df.loc[i,\"RT\"] = tweet.retweet_count\n",
    "        df.loc[i,\"User_location\"] = tweet.user.location\n",
    "        \n",
    "#         df.to_csv(\"TweetDataset.csv\",index=False)\n",
    "        df.to_excel('{}.xlsx'.format(\"TweetDataset\"),index=False)   ## Save as Excel\n",
    "#         parsed = json.loads(df.to_json(orient=\"records\"))\n",
    "#         json.dumps(parsed, indent=4)\n",
    "#         print(json.dumps(parsed, indent=4))\n",
    "        i=i+1\n",
    "        if i>Count:\n",
    "            break\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\r"
     ]
    }
   ],
   "source": [
    "# Call the function to extract the data. pass the topic and filename you want the data to be stored in.\n",
    "Topic=[\"Starbucks\"]\n",
    "get_tweets(Topic , Count=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Clean the Tweet.\n",
    "\n",
    "import re\n",
    "def clean_tweet(tweet):\n",
    "    return ' '.join(re.sub('(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|([RT])', ' ', str(tweet).lower()).split())\n",
    "\n",
    "# We only want the Text so :\n",
    "\n",
    "# (@[A-Za-z0-9]+)   : Delete Anything like @hello @Letsupgrade etc\n",
    "# ([^0-9A-Za-z \\t]) : Delete everything other than text,number,space,tabspace\n",
    "# (\\w+:\\/\\/\\S+)     : Delete https://\n",
    "# ([RT]) : Remove \"RT\" from the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciton to analyze Sentiment\n",
    "\n",
    "from textblob import TextBlob\n",
    "def analyze_sentiment(tweet):\n",
    "    analysis = TextBlob(tweet)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'Positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Pre-process data for Worlcloud:here we are removing the words present in Topic from the Corpus so they dont come in WordCloud.\n",
    "# Ex : Topic is \"Arsenal vs United\", we want to remove \"Arsenal\" \"vs\" \"United\" from the WordCloud.\n",
    "\n",
    "def prepCloud(Topic_text,Topic):\n",
    "    Topic = str(Topic).lower()\n",
    "    Topic=' '.join(re.sub('([^0-9A-Za-z \\t])', ' ', Topic).split())\n",
    "    Topic = re.split(\"\\s+\",str(Topic))\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update(Topic) ### Add our topic in Stopwords, so it doesnt appear in wordClous\n",
    "    ###\n",
    "    text_new = \" \".join([txt for txt in Topic_text.split() if txt not in stopwords])\n",
    "    return text_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweet'] = df['Tweet'].apply(lambda x : clean_tweet(x))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to get the Sentiments\n",
    "\n",
    "df[\"Sentiment\"] = df[\"Tweet\"].apply(lambda x : analyze_sentiment(x))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check Summary of Random Record\n",
    "n = 3\n",
    "print(\"Original tweet:\\n\",df['Tweet'][n])\n",
    "print()\n",
    "print(\"Clean tweet:\\n\",df['clean_tweet'][n])\n",
    "print()\n",
    "print(\"Sentiment of the tweet:\\n\",df['Sentiment'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Summary\n",
    "\n",
    "print(\"Total Tweets Extracted for Topic : {} are : {}\".format(Topic,len(df.Tweet)))\n",
    "print(\"Total Positive Tweets are : {}\".format(len(df[df[\"Sentiment\"]==\"Positive\"])))\n",
    "print(\"Total Negative Tweets are : {}\".format(len(df[df[\"Sentiment\"]==\"Negative\"])))\n",
    "print(\"Total Neutral Tweets are : {}\".format(len(df[df[\"Sentiment\"]==\"Neutral\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.countplot(df[\"Sentiment\"],facecolor=(0, 0, 0, 0),linewidth=5,edgecolor=sns.color_palette(\"dark\", 3))\n",
    "sns.countplot(df[\"Sentiment\"])\n",
    "plt.title(\"Summary of Counts for Total tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Piechart \n",
    "#string, used to label the wedges with their numeric value. The label will be placed inside the wedge. The format string will be fmt%pct.\n",
    "\n",
    "a=len(df[df[\"Sentiment\"]==\"Positive\"])\n",
    "b=len(df[df[\"Sentiment\"]==\"Negative\"])\n",
    "c=len(df[df[\"Sentiment\"]==\"Neutral\"])\n",
    "d=np.array([a,b,c])\n",
    "explode = (0.1, 0.0, 0.1)\n",
    "plt.pie(d,shadow=True,explode=explode,labels=[\"Positive\",\"Negative\",\"Neutral\"],autopct='%1.2f%%');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df[\"Sentiment\"],hue=df.IsVerified)\n",
    "plt.title(\"Summary of Counts for Total tweets,Distributed by if the User has a verified Account or not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with one review:\n",
    "\n",
    "text = df.clean_tweet[2]\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(max_words=10).generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all reviews into one big text and create a Cloud to see which Words are most common in these Tweets.\n",
    "\n",
    "text = \" \".join(review for review in df.clean_tweet)\n",
    "print (\"There are {} words in the combination of all review.\".format(len(text)))\n",
    "\n",
    "\n",
    "# Create stopword list:\n",
    "stopwords = set(STOPWORDS)\n",
    "#stopwords.update([\"drink\", \"now\", \"wine\", \"flavor\", \"flavors\"])  #To add any custom StopWords\n",
    "\n",
    "text_newALL = prepCloud(text,Topic)\n",
    "\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords,max_words=800,max_font_size=70).generate(text_newALL)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title(\"The most frequently used words when searching for {}\".format(Topic),)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_positive = \" \".join(review for review in df[df[\"Sentiment\"]==\"Positive\"].clean_tweet)\n",
    "print (\"There are {} words in the combination of all review.\".format(len(text)))\n",
    "\n",
    "\n",
    "# Create stopword list:\n",
    "stopwords = set(STOPWORDS)\n",
    "#stopwords.update([\"and\", \"now\", \"wine\", \"flavor\", \"flavors\"])  #To add any custom StopWords\n",
    "#text_positive=\" \".join([word for word in text_positive.split() if word not in stopwords])\n",
    "\n",
    "text_new_positive = prepCloud(text_positive,Topic)\n",
    "\n",
    "#stopwords.update([\"drink\", \"now\", \"wine\", \"flavor\", \"flavors\"])  #To add any custom StopWords\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords,max_words=800,max_font_size=70).generate(text_new_positive)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title(\"The most frequently used words when searching for {}\".format(Topic))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Negative reviews into one big text and create a Cloud to see which Words are most common in these Tweets.\n",
    "\n",
    "text_negative = \" \".join(review for review in df[df[\"Sentiment\"]==\"Negative\"].clean_tweet)\n",
    "print (\"There are {} words in the combination of all review.\".format(len(text)))\n",
    "\n",
    "\n",
    "# Create stopword list:\n",
    "stopwords = set(STOPWORDS)\n",
    "#stopwords.update([\"and\", \"now\", \"wine\", \"flavor\", \"flavors\"])  #To add any custom StopWords\n",
    "\n",
    "#text_negative=\" \".join([word for word in text_negative.split() if word not in stopwords])\n",
    "text_new_negative = prepCloud(text_negative,Topic)\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords,max_words=800,max_font_size=70).generate(text_new_negative)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title(\"The most frequently used words when searching for {}\".format(Topic))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "def _connect_mongo(host, port, username, password, db):\n",
    "    \"\"\" A util for making a connection to mongo \"\"\"\n",
    "\n",
    "    if username and password:\n",
    "        mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % (username, password, host, port, db)\n",
    "        conn = MongoClient(mongo_uri)\n",
    "    else:\n",
    "        conn = MongoClient(host, port)\n",
    "\n",
    "\n",
    "    return conn[db]\n",
    "\n",
    "\n",
    "def read_mongo(db, collection, query={}, host='localhost', port=27017, username=None, password=None, no_id=True):\n",
    "    \"\"\" Read from Mongo and Store into DataFrame \"\"\"\n",
    "\n",
    "    # Connect to MongoDB\n",
    "    db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)\n",
    "\n",
    "    # Make a query to the specific DB and Collection\n",
    "    cursor = db[collection].find(query)\n",
    "\n",
    "    # Expand the cursor and construct the DataFrame\n",
    "    df =  pd.DataFrame(list(cursor))\n",
    "\n",
    "    # Delete the _id\n",
    "    if no_id:\n",
    "        del df['_id']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in twitter_df.location:\n",
    "    print(loc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
